{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import json\n",
    "from data_utils import *\n",
    "from model import *\n",
    "from algorithm import *\n",
    "import random\n",
    "import os\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file):\n",
    "    \"\"\"\n",
    "    read the resulting json file\n",
    "    \"\"\"\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_idx = [0,3]\n",
    "nc_idx = [1,2]\n",
    "h, f = logReg(5), logReg(5)\n",
    "h.load_state_dict(torch.load('h_models/h_hiring_lr.pth'))\n",
    "f.load_state_dict(torch.load('f_models/f_lr_hiring.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read best responses\n",
    "GPT_output = read_json('data/4o_hiring_valid.json')\n",
    "GPT_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('data/hiring_data_sample.csv')\n",
    "original['z'] = original['age']\n",
    "X_original = original[['education','YearsCode','PreviousSalary','ComputerSkills','z']]\n",
    "data = Hiring(device='cpu', decision=False)\n",
    "X_original = (X_original - data.mean)/data.std_dev\n",
    "X_original = X_original.to_numpy()\n",
    "X_original = torch.Tensor(X_original)\n",
    "\n",
    "# simulate best response\n",
    "We = torch.tensor([1,1,2,2], dtype = torch.float32)\n",
    "features = ['education','YearsCode','PreviousSalary','ComputerSkills'] \n",
    "efforts_GPT = np.zeros((1000, 4))\n",
    "for i in range(1000):\n",
    "    # read efforts\n",
    "    item = GPT_output[i]\n",
    "    for f in features:\n",
    "        if f in item.keys() and item[f] != {}:\n",
    "            if item[f]['Direction'] == 'decrease':\n",
    "                efforts_GPT[i][features.index(f)] = (-1.0) * item[f]['Effort']\n",
    "            else:\n",
    "                efforts_GPT[i][features.index(f)] = (1.0) * item[f]['Effort']\n",
    "            \n",
    "\n",
    "# get the corresponding effort, x_star, x_improve for ChatGPT\n",
    "x_star_GPT = X_original.clone()\n",
    "x_star_GPT[:,:-1]  = X_original[:,:-1] + torch.Tensor(efforts_GPT)*We\n",
    "x_improve_GPT = x_star_GPT.clone()\n",
    "x_improve_GPT[:,nc_idx] = X_original[:,nc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(x_improve_GPT[0])\n",
    "print(x_star_GPT[0])\n",
    "print(X_original[0])\n",
    "print(np.isnan(x_improve_GPT.detach().numpy()).any())\n",
    "print(np.isnan(x_star_GPT.detach().numpy()).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scores = np.round(h(X_original).detach().numpy(),3).reshape(-1)\n",
    "improve_scores = np.round(h(x_improve_GPT).detach().numpy(),3).reshape(-1)\n",
    "strategy_scores = np.round(h(x_star_GPT).detach().numpy(),3).reshape(-1)\n",
    "print(original_scores.mean())\n",
    "print(improve_scores.mean())\n",
    "print(strategy_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array(original['z'])\n",
    "\n",
    "# save scores\n",
    "np.save('results/4o_hiring_improve_scores_GPT.npy', improve_scores)\n",
    "np.save('results/4o_hiring_strategy_scores_GPT.npy', strategy_scores)\n",
    "\n",
    "# save efforts\n",
    "np.save('results/4o_hiring_efforts_GPT.npy',efforts_GPT)\n",
    "\n",
    "# save groupwise scores\n",
    "np.save('results/4o_hiring_original_scores_0_GPT.npy', original_scores[g==0])\n",
    "np.save('results/4o_hiring_improve_scores_0_GPT.npy', improve_scores[g==0])\n",
    "np.save('results/4o_hiring_strategy_scores_0_GPT.npy', strategy_scores[g==0])\n",
    "\n",
    "np.save('results/4o_hiring_original_scores_1_GPT.npy', original_scores[g==1])\n",
    "np.save('results/4o_hiring_improve_scores_1_GPT.npy', improve_scores[g==1])\n",
    "np.save('results/4o_hiring_strategy_scores_1_GPT.npy', strategy_scores[g==1])\n",
    "\n",
    "# save groupwise efforts\n",
    "np.save('results/4o_hiring_efforts_0_GPT.npy', efforts_GPT[g==0])\n",
    "np.save('results/4o_hiring_efforts_1_GPT.npy', efforts_GPT[g==1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_idx = [0,1]\n",
    "nc_idx = []\n",
    "h, f = logReg(3), logReg(3)\n",
    "h.load_state_dict(torch.load('h_models/h_law_lr.pth'))\n",
    "f.load_state_dict(torch.load('f_models/f_lr_law.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read best responses\n",
    "GPT_output = read_json('data/4o_law_valid.json')\n",
    "GPT_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('data/bar_pass_data_sample.csv')\n",
    "original['z'] = original['sex']\n",
    "X_original = original[['ugpa','lsat','z']]\n",
    "data = LawDataset(device='cpu', decision=False)\n",
    "X_original = (X_original - data.mean)/data.std_dev\n",
    "X_original = X_original.to_numpy()\n",
    "X_original = torch.Tensor(X_original)\n",
    "\n",
    "# simulate best response\n",
    "We = torch.tensor([0.5,0.5], dtype = torch.float32)\n",
    "features = ['UGPA','LSAT']\n",
    "\n",
    "efforts_GPT = np.zeros((1000, 2))\n",
    "for i in range(1000):\n",
    "    # read efforts\n",
    "    item = GPT_output[i]\n",
    "    for f in features:\n",
    "        if f in item.keys() and item[f] != {}:\n",
    "            if item[f]['Direction'] == 'decrease':\n",
    "                efforts_GPT[i][features.index(f)] = (-1.0) * item[f]['Effort']\n",
    "            else:\n",
    "                efforts_GPT[i][features.index(f)] = (1.0) * item[f]['Effort']\n",
    "\n",
    "# get the corresponding effort, x_star, x_improve for ChatGPT\n",
    "x_star_GPT = X_original.clone()\n",
    "x_star_GPT[:,[0,1]]  = X_original[:,[0,1]] + torch.Tensor(efforts_GPT)*We\n",
    "x_improve_GPT = x_star_GPT.clone()\n",
    "x_improve_GPT[:,nc_idx] = X_original[:,nc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(x_improve_GPT[1])\n",
    "print(x_star_GPT[1])\n",
    "print(X_original[1])\n",
    "print(np.isnan(x_improve_GPT.detach().numpy()).any())\n",
    "print(np.isnan(x_star_GPT.detach().numpy()).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scores = np.round(h(X_original).detach().numpy(),3).reshape(-1)\n",
    "improve_scores = np.round(h(x_improve_GPT).detach().numpy(),3).reshape(-1)\n",
    "strategy_scores = np.round(h(x_star_GPT).detach().numpy(),3).reshape(-1)\n",
    "print(original_scores.mean())\n",
    "print(improve_scores.mean())\n",
    "print(strategy_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array(original['z'])\n",
    "\n",
    "# save scores\n",
    "np.save('results/4o_law_improve_scores_GPT.npy', improve_scores)\n",
    "np.save('results/4o_law_strategy_scores_GPT.npy', strategy_scores)\n",
    "\n",
    "# save efforts\n",
    "np.save('results/4o_law_efforts_GPT.npy', efforts_GPT)\n",
    "\n",
    "# save groupwise scores\n",
    "np.save('results/4o_law_original_scores_0_GPT.npy', original_scores[g==1])\n",
    "np.save('results/4o_law_improve_scores_0_GPT.npy', improve_scores[g==1])\n",
    "np.save('results/4o_law_strategy_scores_0_GPT.npy', strategy_scores[g==1])\n",
    "\n",
    "np.save('results/4o_law_original_scores_1_GPT.npy', original_scores[g==2])\n",
    "np.save('results/4o_law_improve_scores_1_GPT.npy', improve_scores[g==2])\n",
    "np.save('results/4o_law_strategy_scores_1_GPT.npy', strategy_scores[g==2])\n",
    "\n",
    "# save groupwise efforts\n",
    "np.save('results/4o_law_efforts_0_GPT.npy', efforts_GPT[g==1])\n",
    "np.save('results/4o_law_efforts_1_GPT.npy', efforts_GPT[g==2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit\n",
    "\n",
    "- specify stategic indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_idx = [2,3]\n",
    "nc_idx = [0,4,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the decision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "h, f = logReg(10), logReg(10)\n",
    "h.load_state_dict(torch.load('h_models/h_credit_lr.pth'))\n",
    "f.load_state_dict(torch.load('f_models/f_LR_credit.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simulate best responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read best responses\n",
    "GPT_output = read_json('data/4o_credit_valid.json')\n",
    "GPT_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CreditDataset(device = 'cpu', decision=False)\n",
    "original = pd.read_csv('data/balanced_test_dataset.csv')\n",
    "original['z'] = np.where(original['age'] > 35, 0, 1)\n",
    "X_original = original.drop(columns = ['age','question', 'qualification_status'])\n",
    "X_original = (X_original - data.mean)/data.std_dev\n",
    "X_original = X_original.to_numpy()\n",
    "X_original = torch.Tensor(X_original)\n",
    "features = ['DebtRatio', 'MonthlyIncome', 'RevolvingUtilizationOfUnsecuredLines','NumberOfOpenCreditLinesAndLoans', 'NumberRealEstateLoansOrLines']\n",
    "# simulate best response\n",
    "We = torch.tensor([2,0.5,0.5,2,2], dtype = torch.float32)\n",
    "\n",
    "efforts_GPT = np.zeros((1000, 5))\n",
    "for i in range(1000):\n",
    "    # read efforts\n",
    "    item = GPT_output[i]\n",
    "    for f in features:\n",
    "        if f in item.keys() and item[f] != {}:\n",
    "            if item[f]['Direction'] == 'decrease':\n",
    "                efforts_GPT[i][features.index(f)] = (-1.0) * item[f]['Effort']\n",
    "            else:\n",
    "                efforts_GPT[i][features.index(f)] = (1.0) * item[f]['Effort']\n",
    "\n",
    "\n",
    "# get the corresponding effort, x_star, x_improve for ChatGPT\n",
    "x_star_GPT = X_original.clone()\n",
    "x_star_GPT[:,[2,3,0,4,6]]  = X_original[:,[2,3,0,4,6]] + torch.Tensor(efforts_GPT)*We\n",
    "x_improve_GPT = x_star_GPT.clone()\n",
    "x_improve_GPT[:,nc_idx] = X_original[:,nc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efforts_GPT.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(x_improve_GPT[0])\n",
    "print(x_star_GPT[0])\n",
    "print(X_original[0])\n",
    "print(np.isnan(x_improve_GPT.detach().numpy()).any())\n",
    "print(np.isnan(x_star_GPT.detach().numpy()).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get score increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scores = np.round(h(X_original).detach().numpy(),3).reshape(-1)\n",
    "improve_scores = np.round(h(x_improve_GPT).detach().numpy(),3).reshape(-1)\n",
    "strategy_scores = np.round(h(x_star_GPT).detach().numpy(),3).reshape(-1)\n",
    "print(original_scores.mean())\n",
    "print(improve_scores.mean())\n",
    "print(strategy_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array(original['z'])\n",
    "\n",
    "# save scores\n",
    "np.save('results/4o_credit_improve_scores_GPT.npy', improve_scores)\n",
    "np.save('results/4o_credit_strategy_scores_GPT.npy', strategy_scores)\n",
    "\n",
    "# save efforts\n",
    "np.save('results/4o_credit_efforts_GPT.npy', efforts_GPT)\n",
    "\n",
    "# save groupwise scores\n",
    "np.save('results/4o_credit_original_scores_0_GPT.npy', original_scores[g==0])\n",
    "np.save('results/4o_credit_improve_scores_0_GPT.npy', improve_scores[g==0])\n",
    "np.save('results/4o_credit_strategy_scores_0_GPT.npy', strategy_scores[g==0])\n",
    "\n",
    "np.save('results/4o_credit_original_scores_1_GPT.npy', original_scores[g==1])\n",
    "np.save('results/4o_credit_improve_scores_1_GPT.npy', improve_scores[g==1])\n",
    "np.save('results/4o_credit_strategy_scores_1_GPT.npy', strategy_scores[g==1])\n",
    "\n",
    "# save groupwise efforts\n",
    "np.save('results/4o_credit_efforts_0_GPT.npy', efforts_GPT[g==0])\n",
    "np.save('results/4o_credit_efforts_1_GPT.npy', efforts_GPT[g==1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACSIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_idx = [0,1]\n",
    "nc_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, f = logReg(4), logReg(4)\n",
    "h.load_state_dict(torch.load('h_models/h_income_lr.pth'))\n",
    "f.load_state_dict(torch.load('f_models/f_LR_income.pth'))\n",
    "\n",
    "# Read best responses\n",
    "GPT_output = read_json('data/4o_income_valid.json')\n",
    "GPT_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('data/ACSIncome_sample_raw.csv')\n",
    "original['z'] = np.where(original['AGEP'] > 35, 0, 1)\n",
    "X_original = original[['SCHL','WKHP','SEX','z']]\n",
    "X_original = X_original.to_numpy()\n",
    "X_original = torch.Tensor(X_original)\n",
    "\n",
    "\n",
    "We = torch.tensor([1,2], dtype = torch.float32)\n",
    "features = ['SCHL','WKHP']\n",
    "efforts_GPT = np.zeros((1000, 2))\n",
    "for i in range(1000):\n",
    "    # read efforts\n",
    "    item = GPT_output[i]\n",
    "    for f in features:\n",
    "        if f in item.keys() and item[f] != {}:\n",
    "            if item[f]['Direction'] == 'decrease':\n",
    "                efforts_GPT[i][features.index(f)] = (-1.0) * item[f]['Effort']\n",
    "            else:\n",
    "                efforts_GPT[i][features.index(f)] = (1.0) * item[f]['Effort']\n",
    "    \n",
    "\n",
    "# get the corresponding effort, x_star, x_improve for ChatGPT\n",
    "x_star_GPT = X_original.clone()\n",
    "x_star_GPT[:,c_idx+nc_idx]  = X_original[:,c_idx+nc_idx] + torch.Tensor(efforts_GPT)*We\n",
    "x_improve_GPT = x_star_GPT.clone()\n",
    "x_improve_GPT[:,nc_idx] = X_original[:,nc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(x_improve_GPT[0])\n",
    "print(x_star_GPT[0])\n",
    "print(X_original[0])\n",
    "print(np.isnan(x_improve_GPT.detach().numpy()).any())\n",
    "print(np.isnan(x_star_GPT.detach().numpy()).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scores = np.round(h(X_original).detach().numpy(),3).reshape(-1)\n",
    "improve_scores = np.round(h(x_improve_GPT).detach().numpy(),3).reshape(-1)\n",
    "strategy_scores = np.round(h(x_star_GPT).detach().numpy(),3).reshape(-1)\n",
    "print(original_scores.mean())\n",
    "print(improve_scores.mean())\n",
    "print(strategy_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array(original['z'])\n",
    "\n",
    "# save scores\n",
    "np.save('results/4o_income_improve_scores_GPT.npy', improve_scores)\n",
    "np.save('results/4o_income_strategy_scores_GPT.npy', strategy_scores)\n",
    "\n",
    "# save efforts\n",
    "np.save('results/4o_income_efforts_GPT.npy', efforts_GPT)\n",
    "\n",
    "# save groupwise scores\n",
    "np.save('results/4o_income_original_scores_0_GPT.npy', original_scores[g==0])\n",
    "np.save('results/4o_income_improve_scores_0_GPT.npy', improve_scores[g==0])\n",
    "np.save('results/4o_income_strategy_scores_0_GPT.npy', strategy_scores[g==0])\n",
    "\n",
    "np.save('results/4o_income_original_scores_1_GPT.npy', original_scores[g==1])\n",
    "np.save('results/4o_income_improve_scores_1_GPT.npy', improve_scores[g==1])\n",
    "np.save('results/4o_income_strategy_scores_1_GPT.npy', strategy_scores[g==1])\n",
    "\n",
    "# save groupwise efforts\n",
    "np.save('results/4o_income_efforts_0_GPT.npy', efforts_GPT[g==0])\n",
    "np.save('results/4o_income_efforts_1_GPT.npy', efforts_GPT[g==1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACSPAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_idx = [1,2]\n",
    "nc_idx = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, f = logReg(4), logReg(4)\n",
    "h.load_state_dict(torch.load('h_models/h_pap_lr.pth'))\n",
    "f.load_state_dict(torch.load('f_models/f_lr_pap.pth'))\n",
    "\n",
    "# Read best responses\n",
    "GPT_output = read_json('data/4o_pap_valid.json')\n",
    "GPT_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('data/ACSPAP_sample.csv')\n",
    "original['z'] = np.where(original['AGEP'] > 35, 0, 1)\n",
    "X_original = original[['WKHP','SCHL','PINCP','z']]\n",
    "data = ACSPAP(device='cpu', decision=False)\n",
    "X_original = (X_original - data.mean)/data.std_dev\n",
    "X_original = X_original.to_numpy()\n",
    "X_original = torch.Tensor(X_original)\n",
    "\n",
    "# simulate best response\n",
    "We = torch.tensor([2,1,1], dtype = torch.float32)\n",
    "features = ['SCHL','PINCP','WKHP']\n",
    "\n",
    "efforts_GPT = np.zeros((1000, 3))\n",
    "for i in range(1000):\n",
    "    # read efforts\n",
    "    item = GPT_output[i]\n",
    "    for f in features:\n",
    "        if f in item.keys() and item[f] != {}:\n",
    "            if item[f]['Direction'] == 'decrease':\n",
    "                efforts_GPT[i][features.index(f)] = (-1.0) * item[f]['Effort']\n",
    "            else:\n",
    "                efforts_GPT[i][features.index(f)] = (1.0) * item[f]['Effort']\n",
    "    \n",
    "\n",
    "# get the corresponding effort, x_star, x_improve for ChatGPT\n",
    "x_star_GPT = X_original.clone()\n",
    "x_star_GPT[:,[1,2,0]]  = X_original[:,[1,2,0]] + torch.Tensor(efforts_GPT)*We\n",
    "x_improve_GPT = x_star_GPT.clone()\n",
    "x_improve_GPT[:,nc_idx] = X_original[:,nc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(x_improve_GPT[1])\n",
    "print(x_star_GPT[1])\n",
    "print(X_original[1])\n",
    "print(np.isnan(x_improve_GPT.detach().numpy()).any())\n",
    "print(np.isnan(x_star_GPT.detach().numpy()).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scores = np.round(h(X_original).detach().numpy(),3).reshape(-1)\n",
    "improve_scores = np.round(h(x_improve_GPT).detach().numpy(),3).reshape(-1)\n",
    "strategy_scores = np.round(h(x_star_GPT).detach().numpy(),3).reshape(-1)\n",
    "print(original_scores.mean())\n",
    "print(improve_scores.mean())\n",
    "print(strategy_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array(original['z'])\n",
    "\n",
    "# save scores\n",
    "np.save('results/4o_PAP_improve_scores_GPT.npy', improve_scores)\n",
    "np.save('results/4o_PAP_strategy_scores_GPT.npy', strategy_scores)\n",
    "\n",
    "# save efforts\n",
    "np.save('results/4o_PAP_efforts_GPT.npy', efforts_GPT)\n",
    "\n",
    "# save groupwise scores\n",
    "np.save('results/4o_PAP_original_scores_0_GPT.npy', original_scores[g==0])\n",
    "np.save('results/4o_PAP_improve_scores_0_GPT.npy', improve_scores[g==0])\n",
    "np.save('results/4o_PAP_strategy_scores_0_GPT.npy', strategy_scores[g==0])\n",
    "\n",
    "np.save('results/4o_PAP_original_scores_1_GPT.npy', original_scores[g==1])\n",
    "np.save('results/4o_PAP_improve_scores_1_GPT.npy', improve_scores[g==1])\n",
    "np.save('results/4o_PAP_strategy_scores_1_GPT.npy', strategy_scores[g==1])\n",
    "\n",
    "# save groupwise efforts\n",
    "np.save('results/4o_PAP_efforts_0_GPT.npy', efforts_GPT[g==0])\n",
    "np.save('results/4o_PAP_efforts_1_GPT.npy', efforts_GPT[g==1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
